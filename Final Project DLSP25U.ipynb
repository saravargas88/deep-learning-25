{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUfJXYK6N3sc"
   },
   "source": [
    "## Introduction\n",
    "---\n",
    "\n",
    "In this programming assignment, we will train a neural net for caption generation.\n",
    "This neural network is made of two components: a feed forward convolutional net (CNN) --- to extract features from the images --- and a recurrent neural net (RNN) --- to output a variable length caption for each input image.\n",
    "We will use MNIST (handwritten digits) dataset for this purpose.\n",
    "\n",
    "## Instructions\n",
    "---\n",
    "\n",
    "You should perform this assignment using Google Colab.\n",
    "* Go through the notebook and ensure that you have answered all questions.\n",
    "* Finally, submit the ipynb `File > Download > Download .ipynb` on Brightspace\n",
    "\n",
    "---\n",
    "This code snippet imports necessary libraries for working with numerical and\n",
    "image data, defines the device for computation (either GPU or CPU), and sets up tools for building and training neural networks using PyTorch, a popular deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sPhXVnjeGT2q"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Use this constant to decide on which device to run the training - On Colab 'cuda:0' and 'cpu' refers to GPU and CPU respectively\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 31\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM7uKdhwRhhe"
   },
   "source": [
    "## Data loading\n",
    "---\n",
    "\n",
    "This code snippet sets up data pre-processing and creates data loaders for training and testing using the MNIST data set, which is commonly used for hand-written digit classification tasks in machine learning.\n",
    "\n",
    "`DataLoader`s abstract outs the loading of a data set and iterating batches of the loaded data set. You can read the documentation to learn more about `DataLoader`s and `Dataset`s [`DataLoader`s and `Dataset`s](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TfTRYSNqJwq4"
   },
   "outputs": [],
   "source": [
    "# Batch options\n",
    "batch_size = 128  # input batch size for training\n",
    "test_batch = 1    # test batch size\n",
    "\n",
    "# Normalizes the input images for a better training\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "# Get train and test loaders to load MNIST data\n",
    "trainset = datasets.MNIST(root='.', train=True, download=True, transform=data_transform)\n",
    "testset = datasets.MNIST(root='.', train=False, download=True, transform=data_transform)\n",
    "\n",
    "# Create the dataloaders on which we can iterate to get the data in batch_size chunks\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(testset, batch_size=test_batch, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfyOh8yrQpXr"
   },
   "source": [
    "# 1 Preprocessing\n",
    "## 1.1 Auxiliary data creation\n",
    "---\n",
    "First, let's setup `labels` and `vocab` for our problem statement. For this,\n",
    "1. MNIST data set, consists of images and their corresponding labels of the form `{0, 1, 2, ‚Ä¶}`.\n",
    "First let's build a list mapping labels to `<b>` $\\phi$ `<e>`, where $\\phi \\in \\lbrace \\texttt{zero}, \\texttt{one}, \\texttt{two}, \\ldots , \\texttt{nine}\\rbrace$.\n",
    "\n",
    "2. Build a lookup table for each unique tokens in the strings. Your `vocab` list, before sorting, should look like: `['<b>', 'z', 'e', 'r', 'o', '<e>', ‚Ä¶]`.\n",
    "\n",
    "Next, complete the following two functions:\n",
    "1.   `label_to_onehot_sequence()` to convert a label to its one-hot sequence, representing the sequence of tokens. You may want to look at [`functional.one_hot` documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html) for reference.\n",
    "2. and `token_idx_to_token()` to convert the list of token indices to their corresponding string of characters.\n",
    "\n",
    "The functions below are heavily annotated so that you understand what each function is doing and how can you use them for your use case.\n",
    "The `assert` is a simple way for testing the correctness of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QPOJLA7ommbp"
   },
   "outputs": [],
   "source": [
    "digits = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "prefix, suffix = '<b>', '<e>'\n",
    "# Create list and Lookup Table ###########################################\n",
    "# TODOüìù: labels = list of sequences of tokens such that it converts\n",
    "#       label to digit into strings\n",
    "\n",
    "labelDict = {\n",
    "    0: \"<b>zero<e>\",\n",
    "    1: \"<b>one<e>\",\n",
    "    2: \"<b>two<e>\",\n",
    "    3: \"<b>three<e>\",\n",
    "    4: \"<b>four<e>\",\n",
    "    5: \"<b>five<e>\",\n",
    "    6: \"<b>six<e>\",\n",
    "    7: \"<b>seven<e>\",\n",
    "    8: \"<b>eight<e>\",\n",
    "    9: \"<b>nine<e>\"}\n",
    "\n",
    "# TODOüìù: vocab = list of all the unique tokens in labels.\n",
    "vocab=['<b>', '<e>', 'e', 'f', 'g', 'h', 'i', 'n', 'o', 'r', 's', 't', 'u', 'v', 'w', 'x', 'z']\n",
    "################################################################################\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "word_size = max([len(digit) for digit in digits]) + 2 # +2 for prefix and suffix\n",
    "\n",
    "# Testing that your lookup tables are setup correctly\n",
    "assert(vocab_size == 17)\n",
    "assert(len(labelDict) == 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hcsxQoFnoMi0"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DO NOT MODIFY THIS CELL #################################\n",
    "################################################################################\n",
    "# Some utility functions to check if your code is working as intended\n",
    "def assert_encoding(actual, expected):\n",
    "  assert(type(actual) == type(torch.tensor(0))) # Your output is not of type tensor, please ensure that your function should output tensors\n",
    "  if(not((expected.numpy() == actual.numpy()).all())):\n",
    "    print('expected: ', expected)\n",
    "    print('actual: ', actual)\n",
    "    assert((expected.numpy() == actual.numpy()).all())\n",
    "\n",
    "# Get the index of a token in the vocab\n",
    "def get_idx(letter):\n",
    "    return vocab.index(letter)\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ODtMG2xbjydT"
   },
   "outputs": [],
   "source": [
    "# Function to convert a list of labels to a list of one-hot matrix\n",
    "def label_to_onehot_sequence(label):\n",
    "    label_index = label.item() # Get the label\n",
    "    #tokens = re.findall(r'<b>|<e>|[a-z]', labels[label_index])\n",
    "    tokens= re.findall(r'<b>|<e>|[a-z]', labelDict[label_index])\n",
    "\n",
    "    #each token is a one hot vector size of vocap \n",
    "    \n",
    "    matrix= torch.zeros(len(tokens), len(vocab)) #number of tokens is row\n",
    "    \n",
    "    for i, token in enumerate(tokens): \n",
    "        row_vec=[ 1 if v ==token else 0 for v in vocab ]\n",
    "        matrix[i]=torch.tensor(row_vec)\n",
    "\n",
    "    return matrix\n",
    "    \n",
    "    # Map label -> one-hot sequence ############################################\n",
    "    # üìù: complete the function converting a single label to its corresponsing one-hot\n",
    "    # in: 9 out: one-hot('<b>nine<e>')\n",
    "\n",
    "    #dictionary look up of label, gets the correspnding token sequence \n",
    "\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    #raise NotImplementedError(\"label_to_onehot_sequence() not implemented\")\n",
    "    \n",
    "\n",
    "assert_encoding(label_to_onehot_sequence(torch.tensor(3)),\n",
    "  torch.tensor([[\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   # <b>\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],   #  t\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   #  h\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],   #  r\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   #  e\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   #  e\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], # <e>\n",
    "    dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_azynUKYsnPH"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DO NOT MODIFY THIS CELL #################################\n",
    "################################################################################\n",
    "def batch_of_labels_to_onehot_matrix(labels):\n",
    "    # Convert labels to one-hot tensors\n",
    "    onehot_inputs = [label_to_onehot_sequence(label) for label in labels]\n",
    "\n",
    "    # Pad the length of string since, matrix operation requires fixed-size rows\n",
    "    max_len = max(len(onehot) for onehot in onehot_inputs)\n",
    "    padded_onehot = pad_sequence(onehot_inputs, batch_first=True, padding_value= 0)\n",
    "    return max_len, padded_onehot\n",
    "\n",
    "# Convert label to label onehot - used in the next to feed argmax input to RNN\n",
    "def label_to_onehot(target):\n",
    "    return F.one_hot(target, num_classes=10).to(torch.float32)\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kKUMCyjYWGin"
   },
   "outputs": [],
   "source": [
    "# To convert token indices predicted by our model back to characters and form the word\n",
    "def token_idx_to_token(input):\n",
    "\n",
    "    # Convert list of token idx to '<b>œÜ<e>' ##################################\n",
    "    # üìù: complete the function to convert a list of token indices to the format '<b>œÜ<e>'\n",
    "    # For each index in the input list, get the corresponding token from vocab, to build the output word\n",
    "    # example input -> [ 0, 3, 6, 13, 2,  1] -> '<b>five<e>'\n",
    "    #                    ‚¨á ‚¨á ‚¨á   ‚¨á ‚¨á  ‚¨á\n",
    "    #                   <b> f  i   v  e  <e>\n",
    "\n",
    "    index_list= input\n",
    "    string= ''\n",
    "    for i in index_list: \n",
    "        string+=(vocab[i])\n",
    "    return str(string)\n",
    "\n",
    "assert(token_idx_to_token([0, 3,  6, 13,  2,  1]) == '<b>five<e>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv1RgBq6g11d"
   },
   "source": [
    "## 1.2 Data Exploration\n",
    "---\n",
    "1. Write code to fetch a batch from the `train_loader` and display the first 10 images along with their respective sequence of tokens which will be of the form `<b>` $\\phi$ `<e>`, where $\\phi \\in \\lbrace \\texttt{zero}, \\texttt{one}, \\texttt{two}, \\ldots, \\texttt{nine}\\rbrace$.\n",
    "\n",
    "Use `matplotlib` and Jupyter notebook's visualisation capabilities. See this [PyTorch tutorial page](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) for hints on how display images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-P9WZJC_gyoc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x250 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the dataset images #################################################\n",
    "# TODOüìù: Fetch a batch from the train_loader and display the first 10 images along\n",
    "#       with the label and the respective token sequence of the form <b>œÜ<e>\n",
    "################################################################################\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualise_images(train_loader, num_images=10):\n",
    "   \n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "    images = images[:num_images]\n",
    "    labels = labels[:num_images]\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 2, 2.5))\n",
    "    for i in range(num_images):\n",
    "        img = images[i].numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))  \n",
    "        \n",
    "        label = labels[i].item()\n",
    "        token_seq = f\"<b>{labelDict[label]}<e>\"\n",
    "\n",
    "        axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[i].set_title(token_seq, fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function to visualize the images along with the label\n",
    "visualise_images(train_loader)  # Assuming `train_loader` is your DataLoader object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTIS2SX4hzNu"
   },
   "source": [
    "## Training and Validation Functions\n",
    "---\n",
    "‚õîÔ∏è‚õîÔ∏è‚õîÔ∏è **Skip this cell for now, and you will have to revisit this cell 3 times, once you have defined your models in Section 2.1, 2.2, and 2.3.**\n",
    "\n",
    "Below are the `train()` and `evaluate()` procedures that you will call using your model to train your model and get accuracy on the validation set respectively. A few lines are omitted in the functions. Complete the following parts:\n",
    "\n",
    "1. Select the appropriate loss function for the tasks.\n",
    "2. Perform the forward pass of the model.\n",
    "3. Compute the loss.\n",
    "4. Zero the gradients of the parameters for no gradient aggregation.\n",
    "5. Compute gradients.\n",
    "6. Optimization step.\n",
    "\n",
    "Please go through both functions thoroughly to understand how accuracy is calculated and how input augmentation is performed before sending it to our models. This way, for other similar tasks, you can emulate similar functions in the future.\n",
    "\n",
    "**`train()`** takes input parameters - **`num_epoch`, `optimiser`, `model`, `dataloader`, and `mode`**.\n",
    "\n",
    "**`mode`** can have the following enums:\n",
    "\n",
    "- `'ENCODER'` for training/evaluating CNN classifier - Section 2.1\n",
    "- `'DECODER'` for training/evaluating the RNN model - Section 2.2\n",
    "- `'MODULAR'` for evaluating the modular approach - Section 2.3\n",
    "- `'E2E'` for training/evaluating the Sections 2.4 and 2.5\n",
    "\n",
    "`evaluate()` takes input parameters - `model`, `dataloader`, and `mode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QnXUEOzhx-0x"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DO NOT MODIFY THIS CELL #################################\n",
    "################################################################################\n",
    "# Utility function to print statistics for debugging as well as modifies target and output for accuracy calculation\n",
    "def trainUtility(batch_idx, epoch, output, target, padded_onehot):\n",
    "    # Printing statistics for easy debugging\n",
    "    if batch_idx == 1 or (epoch != None and epoch%500==0):\n",
    "\n",
    "        \n",
    "        _, pred_idx = torch.max(output, dim=-1)\n",
    "        _, true_idx = torch.max(padded_onehot[:, 1:, :], dim=-1)\n",
    "        \n",
    "        print('acc: ', torch.sum(pred_idx == true_idx)/(pred_idx.shape[0]*pred_idx.shape[1]))\n",
    "        print(pred_idx[0, :], true_idx[0, :])\n",
    "\n",
    "    # Converting output of size (batch_size x max_word_length-1 x vocab_size) -> ((batch_size*(max_word_length-1)) x vocab_size)\n",
    "\n",
    "\n",
    "    output =  output.view(-1, vocab_size)\n",
    "\n",
    "   \n",
    "    # Converting the true labels to ((batch_size*(max_word_length-1)) x vocab_size)\n",
    "    # omitting the first character since, this is not getting predicted by the model\n",
    "    target = padded_onehot[:, 1:, :].reshape(-1, vocab_size)\n",
    "    print(target.shape)\n",
    "\n",
    "    return output, target\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "memn3pIvIdaN"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DO NOT MODIFY THIS CELL #################################\n",
    "################################################################################\n",
    "# We will create a custom dataset for training our RNN in part 2.2, data will be a tuple of\n",
    "# one-hot matrix for each label (batch_size x 10) and corresponding one-hot matrix of words (batch_size x max_word_length x vocab_size)\n",
    "class RnnTrainingDataset(Dataset):\n",
    "    \"\"\"Dataset to get a tuple of one-hot matrix for each label (batch_size x 10)\n",
    "       and corresponding one-hot matrix of words (batch_size x max_word_length - 1 x vocab_size)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        \"\"\"\n",
    "        self.X = label_to_onehot(torch.tensor([x for x in range(10)]))\n",
    "        _, self.Y = batch_of_labels_to_onehot_matrix(torch.tensor([x for x in range(10)]))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10 # There are only 10 words\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx], self.Y[idx])\n",
    "\n",
    "rnn_dataset = RnnTrainingDataset()\n",
    "rnn_dataloader = torch.utils.data.DataLoader(rnn_dataset, batch_size=10, shuffle=True)\n",
    "rnn_testloader = torch.utils.data.DataLoader(rnn_dataset, batch_size=1, shuffle=True)\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qGWgcXa2hyhK"
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Function which does the training for number of epochs and model and type of model passed\n",
    "def train(num_epochs, optimiser, model, dataloader=train_loader, mode='ENCODER'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    # Select Loss ##############################################################\n",
    "    # üìù: criterion = loss function to classify into K classes\n",
    "    ############################################################################\n",
    "\n",
    "    lr_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "          \n",
    "            if mode == 'E2E':  # For 2.4 and 2.5 - CNN-RNN models\n",
    "                # Convert labels to one-hot tensors\n",
    "                _, padded_onehot = batch_of_labels_to_onehot_matrix(target)  # Output size (batch_size x max_word_length x vocab_size)\n",
    "                padded_onehot = padded_onehot.to(device)\n",
    "                \n",
    "                #output = model(data, padded_onehot[:, :-1, :])??\n",
    "                \n",
    "                # Forward pass #####################################################\n",
    "                # TODOüìù: output = forward pass of your model\n",
    "                ####################################################################\n",
    "                raise NotImplementedError(\"Please implement the forward pass of your model\")\n",
    "\n",
    "                # Call trainUtility\n",
    "                output, target = trainUtility(batch_idx, None, output, target, padded_onehot)\n",
    "\n",
    "            elif mode == 'ENCODER':  # For 2.1 - CNN classifier\n",
    "                 output = model.forward(data)\n",
    "\n",
    "                # Forward pass #####################################################\n",
    "                # TODOüìù: output = forward pass of your model\n",
    "                ####################################################################\n",
    "                # raise NotImplementedError(\"Please implement the forward pass of your model\")\n",
    "\n",
    "            elif mode == 'DECODER':  # For 2.2 - Overfitting RNN\n",
    "                # Convert labels to one-hot tensors\n",
    "                padded_onehot = target\n",
    "                batch_idx, seq_len, vocab_size = padded_onehot.shape\n",
    "\n",
    "                # Forward pass using decoder model\n",
    "                output = model(data, padded_onehot[:, :-1, :])\n",
    "                \n",
    "                # Forward pass #####################################################\n",
    "                # TODOüìù: output = forward pass of your model, you will not pass the\n",
    "                #                  end token <e> to your model\n",
    "                ####################################################################\n",
    "                # Call trainUtility\n",
    "                output, target = trainUtility(batch_idx, epoch, output, target, padded_onehot)\n",
    "\n",
    "            \n",
    "\n",
    "            print('DECODER before loss| target shape:', target.shape)\n",
    "            print('DECODER | output shape:', output.shape)\n",
    "\n",
    "            print(\"Output max:\", output.max().item())\n",
    "            print(\"Output min:\", output.min().item())\n",
    "            print(\"Output mean:\", output.mean().item())\n",
    "\n",
    "\n",
    "            if target.ndim == 2 and target.shape[1] > 1:\n",
    "                target_idx = torch.argmax(target, dim=1)\n",
    "            elif target.ndim == 1:\n",
    "                target_idx = target  # already indices\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected target shape: {}\".format(target.shape))\n",
    "\n",
    "\n",
    "            if torch.isnan(output).any():\n",
    "                print(\"‚ùå NaNs found in output before loss!\")\n",
    "                nan_indices = torch.isnan(output)\n",
    "                print(\"Number of NaNs:\", nan_indices.sum())\n",
    "                print(\"Example NaN location:\", torch.nonzero(nan_indices))\n",
    "                print(\"Offending output tensor sample:\", output[0])\n",
    "            \n",
    "            loss = criterion(output, target_idx)  # output shape [N, vocab_size], target_idx shape [N]\n",
    "\n",
    "            print(\"loss:\", loss.item())\n",
    "            print(\"output max:\", output.max().item(), \"min:\", output.min().item())\n",
    "            #loss = criterion(output, target)\n",
    "            \n",
    "            # Loss computation ###################################################\n",
    "            # TODOüìù: loss = evaluate the criterion on the model output\n",
    "            ######################################################################\n",
    "            # raise NotImplementedError(\"evaluate the criterion on the model output\")\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            # Zero grad ##########################################################\n",
    "            # TODOüìù: zero grad the parameters\n",
    "            ######################################################################\n",
    "            # raise NotImplementedError(\"Zero grad the parameters\")\n",
    "\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if torch.isnan(param).any():\n",
    "                    print(f\"NaN detected in {name} weights!\")\n",
    "\n",
    "            \n",
    "            # Compute gradients ##################################################\n",
    "            # TODOüìù: back-propogate loss to calculate the grad weights\n",
    "            ######################################################################\n",
    "            # raise NotImplementedError(\"Back-propogate loss to calculate the grad weights\")\n",
    "\n",
    "            optimiser.step()\n",
    "            # Optimisation step ##################################################\n",
    "            # TODOüìù: perform a single optimization step (weight update)\n",
    "            ######################################################################\n",
    "            # raise NotImplementedError(\"Perform a single optimization step (weight update)\")\n",
    "\n",
    "            if (batch_idx + 1) % 100 == 0 or (mode == 'DECODER' and epoch % 500 == 0):\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, batch_idx + 1, len(dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wFP1MB3Lhn2T"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DO NOT MODIFY THIS CELL #################################\n",
    "################################################################################\n",
    "def evaluate(model, dataloader=test_loader, mode='ENCODER'):\n",
    "    # Never forget to change model to eval mode using eval(), this freezes the model weights for updates\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataloader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        if(mode == 'ENCODER'):\n",
    "          # Predicted output\n",
    "          output = model(data)\n",
    "\n",
    "          # Calculate correct prediction count\n",
    "          pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "          correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        elif(mode == 'DECODER'):\n",
    "          # Predicted output\n",
    "          output = torch.tensor(model.sample(data), device=device)\n",
    "\n",
    "          # True output\n",
    "          _, true = torch.max(target, dim=-1)\n",
    "          # Remove padding\n",
    "          true = torch.tensor(np.concatenate(([0],np.delete(true.cpu().numpy(),\n",
    "                                                            np.argwhere(true.cpu().numpy()==0)))), device=device)\n",
    "\n",
    "          # Calculate correct prediction count\n",
    "          print(\"Comparing output with true:\", output.shape, true.shape)\n",
    "          correct += torch.sum(output == true)/output.shape[0]\n",
    "        elif(mode == 'MODULAR'):\n",
    "          # Predicted output\n",
    "          output = model.sample(data)\n",
    "\n",
    "          # True output\n",
    "          true = [labels[label.item()] for label in target]\n",
    "          true_indices = [0] + [get_idx(c) for c in true[0][3:-3]] + [1]\n",
    "          true = torch.tensor(true_indices)\n",
    "          output = torch.tensor(output)\n",
    "          true = torch.full(output.shape, 20) if output.shape[0] != true.shape[0] else true\n",
    "\n",
    "          # Calculate correct prediction count\n",
    "          correct += torch.sum(output == true)/output.shape[0]\n",
    "        elif(mode == 'E2E'):\n",
    "          # Predicted output\n",
    "          output = torch.tensor(model.sample(data))\n",
    "\n",
    "          # True output\n",
    "          true = [labels[label.item()] for label in target]\n",
    "          true_indices = [0] + [get_idx(c) for c in true[0][3:-3]] + [1]\n",
    "          true = torch.tensor(true_indices)\n",
    "          output = torch.tensor(output)\n",
    "          true = torch.full(output.shape, 20) if output.shape[0] != true.shape[0] else true\n",
    "\n",
    "          # Calculate correct prediction count\n",
    "          correct += torch.sum(output == true)/output.shape[0]\n",
    "\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    print('\\nTest set: Accuracy: {:.0f}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(dataloader.dataset),\n",
    "        100 * correct / len(dataloader.dataset)))\n",
    "\n",
    "    return 100 * correct / len(dataloader.dataset)\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c3ICvb23OMI"
   },
   "source": [
    "# 2 Model creation and training\n",
    "## 2.1 Convolutional net architecture\n",
    "---\n",
    "Follow the following steps to train a CNN classifier and calculate accuracy for your model on the test set:\n",
    "1. Create a model exactly like the architecture given in the prog4 writeup.\n",
    "2. Create a SGD optimiser. You can read about optimisers [here](https://pytorch.org/docs/stable/optim.html).\n",
    "3. Go to previous section and fill out the `train()` procedure for `mode='ENCODER'`.\n",
    "\n",
    "This part expects you to achieve an accuracy of `95%+` on the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QHT3Zskr3Nr-",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uncomment these!!!!!!!!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# options\n",
    "epochs = 10         # number of epochs to train\n",
    "lr = 0.01           # learning rate\n",
    "\n",
    "# Convolutional net architecture ###############################################\n",
    "# üìù: cnnEncoder = cnn model - input: image(batch_size x 1 x img_sz x img_sz)\n",
    "#                                  output: (batch_size, 84)\n",
    "    #NOTES:\n",
    "    #extracting the feature mapper from the input \n",
    "    # it does this by applyign fiters (kernels) to detect different features\n",
    "    #pooling layers: down sample the feature map, only preserve immportant features and reduce dimension\n",
    "    #deeper layers--> cnn captures more features \n",
    "    #flatten layers -->features to make predictions\n",
    "\n",
    "cnnEncoder = nn.Sequential( #input 1x28x28\n",
    "    nn.Conv2d(1, 6, kernel_size=3, stride=1),  # Output: 6x26x26\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),                 # Downsample 6x13x13\n",
    "\n",
    "    nn.Conv2d(6, 16, kernel_size=3, stride=1, padding=1), # Output: 16x11x11\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),                 # Downsample again 16x5x5\n",
    "\n",
    "    nn.Conv2d(16, 120, kernel_size=3, stride=1, padding=1), # Output: 120x3x3\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),                 # Final downsample 120x1x1\n",
    "\n",
    "    nn.Flatten()\n",
    ")\n",
    "\n",
    "# üìù: linearClassifier = linear classifier - input: (batch_size, 84)\n",
    "#                                            output: (batch_size, 10)\n",
    "\n",
    "linearClassifier = nn.Sequential(\n",
    "    nn.Linear(120 * 3 * 3, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "\n",
    "################################################################################\n",
    "#raise NotImplementedError(\"Please implement the CNN encoder and linear classifier\")\n",
    "\n",
    "cnnClassifier = nn.Sequential(cnnEncoder, linearClassifier)\n",
    "enc_optimiser = optim.SGD(cnnClassifier.parameters(), lr=0.01)\n",
    "\n",
    "# Creating Optimiser ###########################################################\n",
    "# TODOüìù: enc_optimiser = create a SGD optimiser for the cnnClassifier\n",
    "# TODOüìù: Fill out the train() procedure for mode='ENCODER' in the previous section\n",
    "################################################################################\n",
    "\n",
    "#raise NotImplementedError(\"Please implement the SGD optimiser\")\n",
    "\n",
    "\n",
    "\n",
    "train(epochs, enc_optimiser, cnnClassifier, mode='ENCODER')\n",
    "acc = evaluate(cnnClassifier, test_loader, mode='ENCODER')\n",
    "\n",
    "assert(acc > 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftoGfiIuFMh_"
   },
   "source": [
    "## 2.2 Recurrent net overfitting\n",
    "---\n",
    "We can define models in two ways, either using `nn.Sequential()` to create a network where PyTorch takes care of `forward()`, or we can create a `class` inhereted from `nn.Module` and define our custom `forward()` and other utility functions. For this section, we will use the latter approach.\n",
    "Follow the next steps to train your RNN model and calculate the accuracy:\n",
    "1. Create a model by completing the class methods where asked.\n",
    "2. Create an optimiser.\n",
    "3. Go to previous section and fill out the `train()` procedure for `mode='DECODER'`.\n",
    "\n",
    "This part expects you to achieve an accuracy of `10/10` on the training set, that is we completely memorise the training data points.\n",
    "\n",
    "\n",
    "You can use the following documentations for reference on [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5Y6hyYp7FG7B"
   },
   "outputs": [],
   "source": [
    "# 1. RNN model for learning sequence of letters\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Create LSTM and Linear objects #######################################\n",
    "        # üìù: self.lstm = LSTM object using input_size, hidden_size and num_layers,\n",
    "        #                     set the batch_first option to True\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )#basically has better recollection of previous steps in backprop\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # üìù: self.fc = Linear layer to convert to get final output with size\n",
    "        #                   (batch_size x vocab_size)\n",
    "        ########################################################################\n",
    "        #raise NotImplementedError(\"Please implement the LSTM and Linear layers\")\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        \n",
    "\n",
    "        # input is batch of one-hot labels of shape (batch_size x 10)\n",
    "        # target is one-hot of sequence of tokens without the last token and is of\n",
    "        # shape (batch_size x max_word_length - 1 x vocab_size)\n",
    "\n",
    "          # Setup ################################################################\n",
    "        # üìù: Using target assign the three variables with the corresponding values of\n",
    "        #         batch_size, max_word_length, and vocab_size\n",
    "\n",
    "      \n",
    "        \n",
    "        batch_size= target.shape[0]\n",
    "        max_word_length = target.shape[1]\n",
    "        vocab_size= target.shape[2]\n",
    "\n",
    "        \n",
    "        if torch.isnan(target).any():\n",
    "            print(\"NaNs in target!\")\n",
    "            \n",
    "        # Check if all rows sum to 1 (valid one-hot)\n",
    "        row_sums = target.sum(dim=2)\n",
    "        if not torch.all(row_sums == 1):\n",
    "            print(\"‚ùó Warning: Some target vectors are not valid one-hot!\")\n",
    "            print(row_sums)\n",
    "\n",
    "        input = input.float()\n",
    "        target = target.float()\n",
    "\n",
    "        #replicated_input = input.unsqueeze(1).repeat(1, max_word_length-1, 1)\n",
    "        replicated_input = input.unsqueeze(1).repeat(1, max_word_length, 1)\n",
    "        \n",
    "        rnn_input = torch.cat((target, replicated_input), dim=2)\n",
    "\n",
    "        # ‚úÖ Add this debug check\n",
    "        if torch.isnan(rnn_input).any():\n",
    "            print(\"‚ùå NaNs in rnn_input!\")\n",
    "        if torch.max(torch.abs(rnn_input)) > 10:\n",
    "            print(\"‚ö†Ô∏è Very large values in rnn_input!\", rnn_input.max())\n",
    "\n",
    "        #print(f\"rnn_input = {rnn_input.shape}\")\n",
    "        # Shape: (batch_size, seq_len, vocab_size + 10)\n",
    "\n",
    "        # Feed-forward behaviour of the model ##################################\n",
    "        # In order to pass two inputs to our LSTM object, we need to concat input and target,\n",
    "        # we will need to change (batch_size x 10) input matrix -> (batch_size x max_word_length-1 x 10)\n",
    "\n",
    "        # üìù: replicated_input = repeat the one-hot input (max_word_length-1)\n",
    "        #         times on 2nd dimension -> (batch_size x max_word_length-1 x 10)\n",
    "        # üìù: rnn_input = concat the target with the replicated_input ->\n",
    "        #         (batch_size x max_word_length-1 x vocab_size+10)\n",
    "\n",
    "        #raise NotImplementedError(\"Feed-forward behaviour of the model\")\n",
    "        out, _ = self.lstm(rnn_input)\n",
    "        if torch.isnan(out).any():\n",
    "            print(\"‚ùå NaN in LSTM output!\")\n",
    "\n",
    "        out = self.fc(out)\n",
    "                \n",
    "        if torch.isnan(out).any():\n",
    "            print(\"‚ùå NaN in FC output!\")\n",
    "\n",
    "\n",
    "        if torch.isnan(out).any():\n",
    "            print(\"‚ùå Decoder output contains NaNs!\")\n",
    "            print(\"Sample output[0]:\", out[0])\n",
    "        # üìù: out = write the feed-forward behaviour of the network\n",
    "        ########################################################################\n",
    "        #raise NotImplementedError(\"Please implement the feed-forward behaviour of the network\")\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Utility function to create next input_rnn using the current token and one-hot label(input)\n",
    "    def create_concatenated_rnn_input(self, token_idx, input_digit):\n",
    "        # input is single one-hot label of shape (1 x 10)\n",
    "        # token_idx is a token index\n",
    "        \n",
    "        #vocab_size = self.fc.out_features\n",
    "        token_onehot = torch.zeros(1, 1, vocab_size).to(input_digit.device)\n",
    "        token_onehot[0, 0, token_idx] = 1\n",
    "    \n",
    "        digit_onehot = input_digit.unsqueeze(1)  # (1, 1, 10)\n",
    "        rnn_input = torch.cat((digit_onehot, token_onehot), dim=2)\n",
    "\n",
    "\n",
    "        #i want the rnn input to have the 10 extra concatenated for the token_onehot thats concatenated. \n",
    "        \n",
    "        # Create rnn_input of (1 x 1 x vocab_size+10) ##########################\n",
    "        # üìù: rnn_input = matrix of (1 x 1 x vocab_size+10), which is the concatenation\n",
    "        #         of the input and the one-hot vector corresponding to the token idx\n",
    "        ########################################################################\n",
    "       # raise NotImplementedError(\"Please implement the rnn_input creation\")\n",
    "\n",
    "        return rnn_input\n",
    "\n",
    "    # This function samples next token given the <b> token until the <e> token\n",
    "    def sample(self, input):\n",
    "        # input is single one-hot label of shape (1 x 10)\n",
    "        output = [get_idx('<b>')] # Output is sequence of token indices, and here we intialise with <b>\n",
    "        rnn_input = self.create_concatenated_rnn_input(get_idx('<b>'), input) # create first input\n",
    "        loopRun = 0\n",
    "        hidden = None\n",
    "        \n",
    "        while True and loopRun < 50:  \n",
    "            # Breaking condition : predicted idx == <e>\n",
    "            # the loopRun < 50 condition prevent infinite looping\n",
    "            out, hidden = self.lstm(rnn_input, hidden)\n",
    "            out = self.fc(out.squeeze(1)) #suppress 1 dim\n",
    "\n",
    "            # Feed-forward behaviour ###########################################\n",
    "            # üìù: out = write the feed-forward behaviour\n",
    "            # Here, since we are working with single token at a time,\n",
    "            # you need to convert out: (1 x 1 x vocab_size) -> (1 x vocab_size)\n",
    "            ####################################################################\n",
    "            \n",
    "            #raise NotImplementedError(\"Please implement the feed-forward behaviour of the network\")\n",
    "\n",
    "            _, pred_idx = torch.max(out, dim=-1) ##gives predicted token\n",
    "\n",
    "            output.append(pred_idx.cpu().numpy()[0].item())\n",
    "\n",
    "       \n",
    "            # Breaking condition ###############################################\n",
    "            # TODOüìù: add breaking condition on predicted token == '<e>'\n",
    "            ####################################################################\n",
    "            if(pred_idx.item() == get_idx('<e>')): #i removed the .get item\n",
    "                break\n",
    "\n",
    "\n",
    "            #rnn_input= create_concatenated_rnn_input(pred_idx, input)\n",
    "            rnn_input = self.create_concatenated_rnn_input(pred_idx.item(), input)\n",
    "\n",
    "            # Prepare next rnn input ###########################################\n",
    "            # üìù: rnn_input = overwrite the rnn_input using the current\n",
    "            #         predicted token for the next iteration\n",
    "            ####################################################################\n",
    "             # raise NotImplementedError(\"Please implement the rnn_input creation\")\n",
    "\n",
    "            loopRun+=1\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "yes\n",
      "hello\n",
      "‚ùó Warning: Some target vectors are not valid one-hot!\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "‚ùå NaN in LSTM output!\n",
      "‚ùå NaN in FC output!\n",
      "‚ùå Decoder output contains NaNs!\n",
      "Sample output[0]: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "acc:  tensor(0.1667)\n",
      "tensor([0, 0, 0, 0, 0, 0]) tensor([10,  2, 13,  2,  7,  1])\n",
      "torch.Size([60, 17])\n",
      "DECODER before loss| target shape: torch.Size([60, 17])\n",
      "DECODER | output shape: torch.Size([60, 17])\n",
      "Output max: nan\n",
      "Output min: nan\n",
      "Output mean: nan\n",
      "‚ùå NaNs found in output before loss!\n",
      "Number of NaNs: tensor(1020)\n",
      "Example NaN location: tensor([[ 0,  0],\n",
      "        [ 0,  1],\n",
      "        [ 0,  2],\n",
      "        ...,\n",
      "        [59, 14],\n",
      "        [59, 15],\n",
      "        [59, 16]])\n",
      "Offending output tensor sample: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "loss: nan\n",
      "output max: nan min: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sara/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4q/tsr3nb5s33gc0n3xh6lwsp040000gn/T/ipykernel_9707/796057026.py\", line 23, in <module>\n",
      "    train(epochs, dec_optimiser, rnnModel, mode='DECODER', dataloader=rnn_dataloader)\n",
      "  File \"/var/folders/4q/tsr3nb5s33gc0n3xh6lwsp040000gn/T/ipykernel_9707/1532280628.py\", line 82, in train\n",
      "    loss = criterion(output, target_idx)  # output shape [N, vocab_size], target_idx shape [N]\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py\", line 1179, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/Users/sara/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 3059, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      " (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:118.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'LogSoftmaxBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Recurrent net overfitting ####################################################\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# TODOüìù: rnnModel = DecoderRNN() model - input: one-hot label output: one-hot matrix of words\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# TODOüìù: dec_optimiser = create a SGD optimiser object\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#raise NotImplementedError(\"Please implement the DecoderRNN and SGD optimiser\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_optimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnnModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDECODER\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrnn_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#acc = evaluate(rnnModel, rnn_testloader, mode='DECODER')\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#assert(acc == 100)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 100\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, optimiser, model, dataloader, mode)\u001b[0m\n\u001b[1;32m     93\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Zero grad ##########################################################\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# TODOüìù: zero grad the parameters\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# raise NotImplementedError(\"Zero grad the parameters\")\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'LogSoftmaxBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "# 2. Create an optimiser\n",
    "# options\n",
    "epochs = 100 #5000         # number of epochs to train\n",
    "lr = 0.0001             # learning rate - TODOüìù: set an appropriate lr\n",
    "hidden_size = 16   # hidden size for rnn - TODOüìù: set from 8, 16, 32, 64 - use the smallest one for which your model works\n",
    "embed_size = 10       # input is onehot labels\n",
    "num_layer = 1         # layers of rnn\n",
    "\n",
    "print('hi')\n",
    "rnnModel = DecoderRNN(input_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size, num_layers=num_layer)\n",
    "print('yes')\n",
    "dec_optimiser = torch.optim.SGD(rnnModel.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "# Recurrent net overfitting ####################################################\n",
    "# TODOüìù: rnnModel = DecoderRNN() model - input: one-hot label output: one-hot matrix of words\n",
    "# TODOüìù: dec_optimiser = create a SGD optimiser object\n",
    "################################################################################\n",
    "#raise NotImplementedError(\"Please implement the DecoderRNN and SGD optimiser\")\n",
    "\n",
    "print('hello')\n",
    "train(epochs, dec_optimiser, rnnModel, mode='DECODER', dataloader=rnn_dataloader)\n",
    "#acc = evaluate(rnnModel, rnn_testloader, mode='DECODER')\n",
    "\n",
    "#assert(acc == 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmSqHzirl2Ny"
   },
   "source": [
    "## 2.3 Modular approach\n",
    "---\n",
    "In this section, we will combine the previously trained CNN and RNN to convert an input image to a sequence of tokens.\n",
    "Here, since both models are pre-trained we just evaluate the performance of this combined model.\n",
    "\n",
    "Create a class `CombinationModel`: with a `sample()` method which takes an image as input and outputs token indices for each predicted letter of size `(batch_size x word_length)`. You can call the sample method of `DecoderRNN` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZEykWBJoXdc"
   },
   "outputs": [],
   "source": [
    "# Modular approach #############################################################\n",
    "# TODOüìù: create a class as mentioned before\n",
    "# TODOüìù: combinationModel = instantiate your created class\n",
    "################################################################################\n",
    "\n",
    "class CombinationModel(nn.Module):\n",
    "    def __init__(self, cnn, rnn): # Everything here is pretrained\n",
    "        super(CombinationModel, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.rnn = rnn\n",
    "\n",
    "    def sample(self, image):\n",
    "        features= self.cnn(image)\n",
    "        return self.rnn.sample(features)\n",
    "\n",
    "combinationModel = CombinationModel(cnnClassifier, rnnModel)\n",
    "\n",
    "acc = evaluate(combinationModel, test_loader, mode='MODULAR')\n",
    "\n",
    "assert(acc > 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVh746TQo97u"
   },
   "source": [
    "## 2.4 Transfer learning\n",
    "---\n",
    "\n",
    "In this section, we will use the `cnnEncoder` that we trained in section 2.1 to get the image embeddings which we will use instead of the one-hot label we used in section 2.2. Complete the following methods using section 2.2 as reference:\n",
    "1. Create a `DecoderRNN2` model;\n",
    "2. Create a `DigitDecoder` model (you may want to look at the methods of `DecoderRNN` from section 2.2);\n",
    "3. Instantiate an object of `DigitDecoder`;\n",
    "4. Create a SGD optimiser for the `DecoderRNN2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aNs_xKALtnEX"
   },
   "outputs": [],
   "source": [
    "# 1. RNN model for learning sequence of letters\n",
    "class DecoderRNN2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN2, self).__init__()\n",
    "        # Initialisation of the class ##########################################\n",
    "        # TODOüìù: create LSTM and linear layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "            \n",
    "        ########################################################################\n",
    "        #raise NotImplementedError(\"Please implement the LSTM and Linear layers\")\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Concatenated vector of image embedding and one-hot of the whole\n",
    "        # sequence of tokens without the last one of size (batch_size x max_word_length-1 x vocab_size+embed_size)\n",
    "        # Feed-forward behaviour ###############################################\n",
    "        # TODOüìù: write the feed-forward behaviour of the model\n",
    "\n",
    "        output,  hidden = self.lstm(input)\n",
    "        output= self.fc(output)\n",
    "        return output\n",
    "        \n",
    "        ########################################################################\n",
    "        #raise NotImplementedError(\"Please implement the feed-forward behaviour of the network\")\n",
    "\n",
    "    def single_forward(self, input, hidden):\n",
    "        # This function is called by DigitDecoder to output a single token given\n",
    "        # the previous token along with the image embedding and the corresponding hidden tensor\n",
    "\n",
    "        out, hidden = self.lstm(input, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "        # Prediction behaviour #################################################\n",
    "        # TODOüìù: do a forward pass of the model\n",
    "        # TODOüìù: return output and new hidden\n",
    "        ########################################################################\n",
    "        #raise NotImplementedError(\"Please implement the forward pass of the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "dKnUTAFg_hix"
   },
   "outputs": [],
   "source": [
    "# 2. Model to predict next words given the image\n",
    "class DigitDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(DigitDecoder, self).__init__()\n",
    "        # Initialise the model #################################################\n",
    "        # TODOüìù: self.encoder = use the cnnEncoder\n",
    "        # TODOüìù: self.decoder = instantiation of DecoderRNN2\n",
    "        ########################################################################\n",
    "        self.encoder = cnnEncoder\n",
    "        self.decoder = DecoderRNN2(input_size + vocab_size, hidden_size, vocab_size, num_layers)\n",
    "    \n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Feed-forward behaviour of the model ##################################\n",
    "        # TODOüìù: try to emulate forward() from section 2.2\n",
    "        # Some extra work is needed: calculate image embedding using\n",
    "        # the self.encoder\n",
    "\n",
    "        embeddings = self.encoder(x)\n",
    "    \n",
    "        # Reshape embeddings for sequence\n",
    "        batch_size, max_word_length = y.shape[0], y.shape[1]\n",
    "        embeddings = embeddings.unsqueeze(1).repeat(1, max_word_length-1, 1)\n",
    "        \n",
    "        # Concatenate with y\n",
    "        rnn_input = torch.cat((y[:, :-1, :], embeddings), dim=2)\n",
    "        \n",
    "        # Forward through RNN\n",
    "        output = self.decoder(rnn_input)\n",
    "        \n",
    "        return output\n",
    "        ########################################################################\n",
    "        #raise NotImplementedError(\"Please implement the forward pass of the model\")\n",
    "\n",
    "    def sample(self, x):\n",
    "        # Sampling the output string one token at a time #######################\n",
    "        # TODOüìù: try to emulate sample() method from section 2.2\n",
    "        # Some extra work is needed: calculate image embedding using\n",
    "        # the self.encoder\n",
    "        \n",
    "        ########################################################################\n",
    "        raise NotImplementedError(\"Please implement the sample method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "VLe3qH_-xlLS"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Please implement the DigitDecoder and SGD optimiser",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m tra_optimiser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Transfer learning ############################################################\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# TODOüìù: tranferLearningModel = create an object of DigitDecoder\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# TODOüìù: tra_optimiser = create a SGD optimiser object using just the DecoderRNN2 parameters\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease implement the DigitDecoder and SGD optimiser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m train(epochs, tra_optimiser, tranferLearningModel, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE2E\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m acc \u001b[38;5;241m=\u001b[39m evaluate(tranferLearningModel, test_loader, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE2E\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Please implement the DigitDecoder and SGD optimiser"
     ]
    }
   ],
   "source": [
    "# 3. and 4. model instantiation and optimiser creation\n",
    "epochs = 10           # number of epochs to train\n",
    "lr = 0.1              # learning rate\n",
    "hidden_size = None    # hidden size for rnn - set from 8, 16, 32, 64 - use the smallest one for which your model works\n",
    "num_layer = 1         # layers of rnn\n",
    "embed_size = 84\n",
    "\n",
    "tranferLearningModel = None\n",
    "tra_optimiser = None\n",
    "# Transfer learning ############################################################\n",
    "# TODOüìù: tranferLearningModel = create an object of DigitDecoder\n",
    "# TODOüìù: tra_optimiser = create a SGD optimiser object using just the DecoderRNN2 parameters\n",
    "################################################################################\n",
    "raise NotImplementedError(\"Please implement the DigitDecoder and SGD optimiser\")\n",
    "\n",
    "train(epochs, tra_optimiser, tranferLearningModel, mode='E2E')\n",
    "acc = evaluate(tranferLearningModel, test_loader, mode='E2E')\n",
    "\n",
    "assert(acc > 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k94y4Bb-yhL7"
   },
   "source": [
    "## 2.5 End-to-end training\n",
    "---\n",
    "Finally, we will remove all the restrictions and train the `DigitDecoder` on both CNN as well as `DecoderRNN2` parameters.\n",
    "\n",
    "Follow the same steps as sec 2.4.\n",
    "1. Create a new class `DigitDecoder2` similar to `DigitDecoder` except here instead of using the pre-trained `cnnEncoder`, instantiate a new encoder with the same hyper-parameter.\n",
    "1. Instantiate an object of `DigitDecoder2`.\n",
    "2. Create an optimiser object using the `DigitDecoder2` weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "TtbsQ6qMygTb"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 74)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:74\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise NotImplementedError(\"Please implement the DigitDecoder2 class\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class DigitDecoder2(nn.Module):\n",
    "    def __init__(self, input_size=84, hidden_size=32, num_layers=1, num_classes=10):\n",
    "        super(DigitDecoder2, self).__init__()\n",
    "        \n",
    "        # Create a new CNN encoder (similar structure to the one in section 2.1)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84)\n",
    "        )\n",
    "        \n",
    "        # RNN decoder\n",
    "        self.decoder = DecoderRNN2(input_size + vocab_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # Similar to DigitDecoder\n",
    "        embeddings = self.encoder(x)\n",
    "        batch_size, max_word_length = y.shape[0], y.shape[1]\n",
    "        embeddings = embeddings.unsqueeze(1).repeat(1, max_word_length-1, 1)\n",
    "        rnn_input = torch.cat((y[:, :-1, :], embeddings), dim=2)\n",
    "        output = self.decoder(rnn_input)\n",
    "        return output\n",
    "        \n",
    "    def sample(self, x):\n",
    "        # Similar to DigitDecoder\n",
    "        embeddings = self.encoder(x)\n",
    "        \n",
    "        # Start with <b> token\n",
    "        tokens = [get_idx('<b>')]\n",
    "        token_idx = get_idx('<b>')\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = None\n",
    "        \n",
    "        # Loop until <e> or max length\n",
    "        for _ in range(50):\n",
    "            # Create one-hot for current token\n",
    "            token_one_hot = F.one_hot(torch.tensor(token_idx), num_classes=vocab_size).float().to(device)\n",
    "            token_one_hot = token_one_hot.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # Combine with image embedding\n",
    "            rnn_input = torch.cat((token_one_hot, embeddings.unsqueeze(1)), dim=2)\n",
    "            \n",
    "            # Get next token prediction\n",
    "            out, hidden = self.decoder.single_forward(rnn_input, hidden)\n",
    "            _, pred_idx = torch.max(out, dim=-1)\n",
    "            pred_idx = pred_idx[0][0].item()\n",
    "            \n",
    "            # Add to output sequence\n",
    "            tokens.append(pred_idx)\n",
    "            \n",
    "            # Break if end token\n",
    "            if pred_idx == get_idx('<e>'):\n",
    "                break\n",
    "                \n",
    "            # Update for next iteration\n",
    "            token_idx = pred_idx\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "  # Fill out the class methods #################################################\n",
    "  # TODOüìù: Create a new DigitDecoder2 class, with the difference in self.encoder,\n",
    "  #         where you need to a new cnn encoder with same hyper-parameter as in\n",
    "  #         section 2.1 without the last layer.\n",
    "  ##############################################################################\n",
    "  #raise NotImplementedError(\"Please implement the DigitDecoder2 class\")\n",
    "\n",
    "# options\n",
    "epochs = None       # number of epochs to train - TODOüìù: set number of epochs\n",
    "lr = None           # learning rate - TODOüìù: set appropriate lr\n",
    "hidden_size = None  # hidden size for rnn - TODOüìù: you may use the last lr\n",
    "num_layer = None    # layers of rnn - TODOüìù: decide on number of layers to have in the rnn\n",
    "\n",
    "e2eModel = DigitDecoder2()\n",
    "e2eOptimiser = None\n",
    "# End-to-end training ##########################################################\n",
    "# TODOüìù: e2eModel = create an object of DigitDetector\n",
    "# TODOüìù: e2eOptimiser = create an optimiser object using DigitDetector parameters\n",
    "################################################################################\n",
    "raise NotImplementedError(\"Please implement the DigitDecoder2 and SGD optimiser\")\n",
    "\n",
    "train(epochs, e2eOptimiser, e2eModel, mode='E2E')\n",
    "acc = evaluate(e2eModel, test_loader, mode='E2E')\n",
    "\n",
    "assert(acc > 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD8pta-Tzkm0"
   },
   "source": [
    "# 3 Inference\n",
    "## 3.1 Evaluation and models comparison\n",
    "---\n",
    "Now that we are done with training our models and hopefully have got near perfect accuracies on all our tasks, its time to see the predictions of our model.\n",
    "1. Complete the function `infer()` that displays any 10 images along with their predicted texts. We will call the function for the following models:\n",
    "    1. `combinationModel`\n",
    "    2. `tranferLearningModel`\n",
    "    3. `e2eModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "08xib6gw2s9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombinationModel output: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 10, got 27",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 42\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluation and models comparison #############################################\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# TODOüìù: Complete the Infer(.) function\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#raise NotImplementedError(\"Please implement the infer function\")\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinationModel output: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombinationModel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranferLearningModel output: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m infer(tranferLearningModel)\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m         pred_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         pred_text \u001b[38;5;241m=\u001b[39m token_idx_to_token(pred_tokens)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# For models without sample method\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m, in \u001b[0;36mCombinationModel.sample\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m     13\u001b[0m     features\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(image)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 126\u001b[0m, in \u001b[0;36mDecoderRNN.sample\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    121\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m loopRun \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m:  \n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# Breaking condition : predicted idx == <e>\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# the loopRun < 50 condition prevent infinite looping\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#suppress 1 dim\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Feed-forward behaviour ###########################################\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# üìù: out = write the feed-forward behaviour\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Here, since we are working with single token at a time,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m#raise NotImplementedError(\"Please implement the feed-forward behaviour of the network\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:874\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    871\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m--> 874\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:789\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    785\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    786\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    787\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    788\u001b[0m                        ):\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    791\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    793\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:239\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 10, got 27"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def infer(model):\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images = images.to(device)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(10):\n",
    "        # Get the image and make prediction\n",
    "        img = images[i].cpu().squeeze().numpy()\n",
    "        \n",
    "        # get prediction from model\n",
    "        with torch.no_grad():\n",
    "            if hasattr(model, 'sample'):\n",
    "                pred_tokens = model.sample(images[i:i+1])\n",
    "                pred_text = token_idx_to_token(pred_tokens)\n",
    "            else:\n",
    "                # For models without sample method\n",
    "                output = model(images[i:i+1])\n",
    "                _, pred_idx = torch.max(output, 1)\n",
    "                pred_text = labelDict[pred_idx.item()]\n",
    "        \n",
    "        # get true label\n",
    "        true_label = labels[i].item()\n",
    "        true_text = labelDict[true_label]\n",
    "        \n",
    "        # plot \n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"True: {true_text}\\nPred: {pred_text}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Evaluation and models comparison #############################################\n",
    "# TODOüìù: Complete the Infer(.) function\n",
    "################################################################################\n",
    "#raise NotImplementedError(\"Please implement the infer function\")\n",
    "\n",
    "print('CombinationModel output: ')\n",
    "infer(combinationModel)\n",
    "print('TranferLearningModel output: ')\n",
    "infer(tranferLearningModel)\n",
    "print('E2eModel output: ')\n",
    "infer(e2eModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEPfnRFi0wRM"
   },
   "source": [
    "## Conclusion\n",
    "Congratulations!!! You just trained an image caption generator. Even though the problem on which we trained our model were simple MNIST images, the concepts and methods we learnt can be extended to train for harder datasets like [Flicker8k_Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k) or [Coco_Dataset](https://cocodataset.org/#home) as well.\n",
    "\n",
    "I would highly recomment that you try with these datasets and see how well different approaches are performing, you may need to tweak with more complex and deeper models in order to get satisfactory accuracy."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rUfJXYK6N3sc",
    "oM7uKdhwRhhe",
    "QfyOh8yrQpXr",
    "2c3ICvb23OMI",
    "xD8pta-Tzkm0"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
